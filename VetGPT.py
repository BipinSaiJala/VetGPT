# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSJujnxf4D8NrSIrkxg0Fvd5kZIWm0zd
"""

!pip install selenium

import re
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Function to scrape data using Selenium
def scrape_vet_data():
    urls = [
        # List of URLs as defined
        "https://veterinarypartner.vin.com/default.aspx?pId=19239&catId=102887",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=6386334",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=10517463&ind=681&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4952664&ind=693&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=102903&id=12296225&ind=678&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=6386334&ind=680&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=10517463&ind=681&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951475&ind=682&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951536&ind=683&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=12289434&ind=684&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4952543&ind=685&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951977&ind=686&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951977&ind=686&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951815&ind=687&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951526&ind=688&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4951868&ind=689&objTypeID=1007",
    ]

    # Selenium Chrome driver options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=chrome_options)
    texts = []

    for url in urls:
        try:
            driver.get(url)
            time.sleep(3)  # Wait for the page to load completely
            soup = BeautifulSoup(driver.page_source, 'html.parser')

            #paragraphs = soup.find_all(['p', 'div', 'span'])
            paragraphs = soup.find_all(['p', 'div', 'span', 'h1', 'h2', 'h3', 'ul', 'ol'])
            for p in paragraphs:
                text = p.get_text(strip=True)
                # Filter paragraphs with relevant content
                if len(text.split()) > 15 and re.search(r'\b(is|are|has|causes|may|and|or|because|should|must|will)\b', text, re.IGNORECASE):
                    texts.append(text)
        except Exception as e:
            print(f"Error accessing {url}: {e}")

    driver.quit()

    print(f"Scraped {len(texts)} relevant paragraphs.")
    return texts

# Simple tokenizer
def simple_tokenizer(text):
    tokens = re.findall(r'\w+', text.lower())
    return [word_to_index.get(token, 1) for token in tokens]

# Reverse tokenizer
def reverse_tokenizer(index):
    return index_to_word.get(index, '')

# Custom dataset class
class VetDataset(Dataset):
    def __init__(self, texts, tokenizer, max_seq_len):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = self.tokenizer(self.texts[idx])
        padded_tokens = tokens + [0] * (self.max_seq_len - len(tokens))
        return torch.tensor(padded_tokens[:self.max_seq_len])

# Define VetGPT model
class VetGPT(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads, ffn_hidden_dim, max_seq_len):
        super(VetGPT, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.dropout = nn.Dropout(0.1)  # Added dropout for regularization
        self.transformer_blocks = nn.ModuleList(
            [nn.TransformerEncoderLayer(d_model, n_heads, ffn_hidden_dim, dropout=0.1) for _ in range(n_layers)]
        )
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.dropout(x)
        for transformer in self.transformer_blocks:
            x = transformer(x)
        return self.fc_out(x)

# Training function
def train_model(model, data_loader, num_epochs, device):
    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Reduced learning rate
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0
        for batch in data_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            output = model(batch[:, :-1])
            loss = criterion(output.reshape(-1, output.size(-1)), batch[:, 1:].reshape(-1))
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader)}")

# Top-P (nucleus) sampling generation function
def generate_text_top_p_sampling(model, start_text, max_len=50, top_p=0.9):
    model.eval()
    with torch.no_grad():
        input_ids = torch.tensor([simple_tokenizer(start_text)]).to(device)
        generated = input_ids

        for _ in range(max_len - len(input_ids[0])):
            output = model(generated)
            next_token_logits = output[:, -1, :].squeeze(0)
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            indices_to_remove = cumulative_probs > top_p
            sorted_logits[indices_to_remove] = float('-inf')
            probs = F.softmax(sorted_logits, dim=-1)

            probs[torch.isnan(probs)] = 0
            probs[probs < 0] = 0
            if probs.sum() == 0:
                probs = torch.ones_like(probs) / len(probs)

            next_token = sorted_indices[torch.multinomial(probs, 1)]
            generated = torch.cat((generated, next_token.view(1, 1).to(device)), dim=1)

        output_text = ' '.join(reverse_tokenizer(idx.item()) for idx in generated[0])
    return output_text

# Main script
if __name__ == "__main__":
    # Scrape data
    texts = scrape_vet_data()
    if not texts:
        raise ValueError("No data scraped; check the scraping function and URLs.")
    print("Sample scraped paragraph:", texts[0])

    # Prepare token mappings
    word_to_index = {word: idx for idx, word in enumerate(set(' '.join(texts).split()), start=2)}
    index_to_word = {idx: word for word, idx in word_to_index.items()}
    index_to_word[0], index_to_word[1] = '<PAD>', '<UNK>'

    # Hyperparameters
    max_seq_len = 20
    vocab_size = len(word_to_index) + 2
    d_model = 512
    n_layers = 4
    n_heads = 8
    ffn_hidden_dim = 1024
    batch_size = 4
    num_epochs = 30

    # Create dataset and dataloader
    dataset = VetDataset(texts, simple_tokenizer, max_seq_len)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Initialize and train model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = VetGPT(vocab_size, d_model, n_layers, n_heads, ffn_hidden_dim, max_seq_len).to(device)
    train_model(model, data_loader, num_epochs, device)

    # Save model
    torch.save(model.state_dict(), 'vetgpt_model.pth')
    print("Model saved as 'vetgpt_model.pth'.")

    # Generate example text
    prompt = "How Red Blood Cells are normally removed from the body"
    response = generate_text_top_p_sampling(model, prompt, max_len=500, top_p=1.0)
    print("Generated text:", response)

import re
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Function to scrape data using Selenium
def scrape_website_data():
    urls = [
        # List of URLs as defined
        "https://veterinarypartner.vin.com/default.aspx?pId=19239&catId=102887",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=6386334",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=10517463&ind=681&objTypeID=1007",
        "https://veterinarypartner.vin.com/default.aspx?pid=19239&catId=254055&id=4952664&ind=693&objTypeID=1007",
        # Additional URLs...
    ]

    # Selenium Chrome driver options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=chrome_options)
    extracted_texts = []

    for url in urls:
        try:
            driver.get(url)
            time.sleep(3)  # Wait for the page to load completely
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            content_elements = soup.find_all(['p', 'div', 'span', 'h1', 'h2', 'h3', 'ul', 'ol'])
            for element in content_elements:
                text = element.get_text(strip=True)
                # Filter relevant content
                if len(text.split()) > 15 and re.search(r'\b(is|are|has|causes|may|and|or|because|should|must|will)\b', text, re.IGNORECASE):
                    extracted_texts.append(text)
        except Exception as e:
            print(f"Error accessing {url}: {e}")

    driver.quit()
    print(f"Scraped {len(extracted_texts)} relevant paragraphs.")
    return extracted_texts

# Tokenizer function
def basic_tokenizer(text):
    tokens = re.findall(r'\w+', text.lower())
    return [word_to_id.get(token, 1) for token in tokens]

# Reverse tokenizer function
def reverse_token(index):
    return id_to_word.get(index, '')

# Custom dataset class
class VeterinaryDataset(Dataset):
    def __init__(self, texts, tokenizer, max_sequence_length):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_sequence_length = max_sequence_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = self.tokenizer(self.texts[idx])
        padded_tokens = tokens + [0] * (self.max_sequence_length - len(tokens))
        return torch.tensor(padded_tokens[:self.max_sequence_length])

# Define the VeterinaryGPT model
class VeterinaryGPT(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_layers, num_heads, feedforward_dim, max_sequence_length):
        super(VeterinaryGPT, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.dropout = nn.Dropout(0.1)  # Added dropout for regularization
        self.transformer_layers = nn.ModuleList(
            [nn.TransformerEncoderLayer(embedding_dim, num_heads, feedforward_dim, dropout=0.1) for _ in range(num_layers)]
        )
        self.output_layer = nn.Linear(embedding_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.dropout(x)
        for transformer in self.transformer_layers:
            x = transformer(x)
        return self.output_layer(x)

# Training function
def train_gpt_model(model, data_loader, num_epochs, device):
    optimizer = optim.Adam(model.parameters(), lr=0.0005)
    loss_function = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch in data_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            output = model(batch[:, :-1])
            loss = loss_function(output.reshape(-1, output.size(-1)), batch[:, 1:].reshape(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader)}")

# Text generation function using Top-P sampling
def generate_text_with_top_p_sampling(model, initial_text, max_length=50, top_p=0.9):
    model.eval()
    with torch.no_grad():
        input_ids = torch.tensor([basic_tokenizer(initial_text)]).to(device)
        generated_text = input_ids

        for _ in range(max_length - len(input_ids[0])):
            output = model(generated_text)
            next_token_logits = output[:, -1, :].squeeze(0)
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            indices_to_remove = cumulative_probs > top_p
            sorted_logits[indices_to_remove] = float('-inf')
            probabilities = F.softmax(sorted_logits, dim=-1)

            probabilities[torch.isnan(probabilities)] = 0
            probabilities[probabilities < 0] = 0
            if probabilities.sum() == 0:
                probabilities = torch.ones_like(probabilities) / len(probabilities)

            next_token = sorted_indices[torch.multinomial(probabilities, 1)]
            generated_text = torch.cat((generated_text, next_token.view(1, 1).to(device)), dim=1)

        output_text = ' '.join(reverse_token(idx.item()) for idx in generated_text[0])
    return output_text

# Main script
if __name__ == "__main__":
    # Scrape data
    paragraphs = scrape_website_data()
    if not paragraphs:
        raise ValueError("No data scraped; check the scraping function and URLs.")
    print("Sample scraped paragraph:", paragraphs[0])

    # Prepare token mappings
    word_to_id = {word: idx for idx, word in enumerate(set(' '.join(paragraphs).split()), start=2)}
    id_to_word = {idx: word for word, idx in word_to_id.items()}
    id_to_word[0], id_to_word[1] = '<PAD>', '<UNK>'

    # Hyperparameters
    max_sequence_length = 20
    vocabulary_size = len(word_to_id) + 2
    embedding_dim = 512
    num_layers = 4
    num_heads = 8
    feedforward_dim = 1024
    batch_size = 4
    num_epochs = 30

    # Create dataset and dataloader
    dataset = VeterinaryDataset(paragraphs, basic_tokenizer, max_sequence_length)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Initialize and train model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = VeterinaryGPT(vocabulary_size, embedding_dim, num_layers, num_heads, feedforward_dim, max_sequence_length).to(device)
    train_gpt_model(model, data_loader, num_epochs, device)

    # Save model
    torch.save(model.state_dict(), 'veterinary_gpt_model.pth')
    print("Model saved as 'veterinary_gpt_model.pth'.")

    # Generate example text
    prompt = "Infections of cat"
    generated_response = generate_text_with_top_p_sampling(model, prompt, max_length=500, top_p=1.0)
    print("Generated text:", generated_response)